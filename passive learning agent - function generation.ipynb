{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Connect3Environment():\n",
    "    def __init__(self, n_rows, n_cols, value_fun):\n",
    "        self.n_cols = n_cols\n",
    "        self.n_rows = n_rows\n",
    "        self.board = self.initialize_board()\n",
    "        print(self.board)\n",
    "        \n",
    "        self.value_fun = value_fun\n",
    "    \n",
    "    def initialize_board(self):\n",
    "        return np.zeros((self.n_rows, self.n_cols))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = self.initialize_board()\n",
    "        \n",
    "    def get_all_states(self):\n",
    "        return [*self.value_fun]\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        reshaped_state = np.array(state).reshape(self.n_rows, self.n_cols)\n",
    "        #print(reshaped_state)\n",
    "        for row in range(4):\n",
    "            for col in range(5):\n",
    "                if reshaped_state[row][col] == 0:\n",
    "                    continue\n",
    "                player = reshaped_state[row][col]\n",
    "                if (col <= 2 and all(reshaped_state[row][col+i] == player for i in range(3))) or \\\n",
    "                   (row <= 1 and all(reshaped_state[row+i][col] == player for i in range(3))) or \\\n",
    "                   (row <= 1 and col <= 2 and all(reshaped_state[row+i][col+i] == player for i in range(3))) or \\\n",
    "                   (row >= 2 and col <= 2 and all(reshaped_state[row-i][col+i] == player for i in range(3))):\n",
    "                    return True\n",
    "        # checking for draw also\n",
    "        return not any(0 in row for row in reshaped_state)\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        reshaped_state = np.array(state).reshape(self.n_rows, self.n_cols)\n",
    "        return [col for col in range(5) if reshaped_state[0, col] == 0]\n",
    "    \n",
    "    def calculate_turn(self, state):\n",
    "        reshaped_state = np.array(state).reshape(self.n_rows, self.n_cols)\n",
    "        player1_count = np.sum(reshaped_state==1)\n",
    "        player2_count = np.sum(reshaped_state==2)\n",
    "        \n",
    "        if player1_count == player2_count:\n",
    "            return 1\n",
    "        elif player1_count > player2_count:\n",
    "            return 2\n",
    "        else:\n",
    "            print(reshaped_state)\n",
    "            return None\n",
    "    \n",
    "    def get_next_states(self, state, action):\n",
    "        #reshaped_state = np.array(state).reshape(self.n_rows, self.n_cols)\n",
    "        probs = {}\n",
    "        #self.board = state\n",
    "        next_state = self.step(state, action)\n",
    "        valid_actions_for_state = self.get_possible_actions(next_state)\n",
    "        n_valid = len(valid_actions_for_state)\n",
    "        \n",
    "        if len(valid_actions_for_state) == 0:\n",
    "            return probs\n",
    "        else:\n",
    "            for actions in valid_actions_for_state:\n",
    "                next_next = self.step(next_state, actions)\n",
    "                next_state_tuple = tuple(next_next.flatten())\n",
    "                probs[next_state_tuple] = 1/n_valid\n",
    "        \n",
    "        return probs                \n",
    "            \n",
    "        \n",
    "    def step(self, state, action):\n",
    "        reshaped_board = np.array(state).reshape(self.n_rows, self.n_cols)\n",
    "        player = self.calculate_turn(reshaped_board)\n",
    "        \n",
    "        for row in range(3, -1, -1):\n",
    "            if reshaped_board[row, action] == 0:\n",
    "                reshaped_board[row, action] = player\n",
    "                break\n",
    "        \n",
    "        #self.board = reshaped_board\n",
    "        \n",
    "        return reshaped_board\n",
    "    \n",
    "    def get_reward(self, state, action, next_state):\n",
    "        reshaped_state = np.array(state).reshape(self.n_rows, self.n_cols)\n",
    "        assert action in self.get_possible_actions(reshaped_state)\n",
    "        \n",
    "        # draw\n",
    "        if not any(0 in row for row in reshaped_state):\n",
    "            return 0\n",
    "        # current state is terminal - lost game\n",
    "        elif self.is_terminal(state):\n",
    "            return -1\n",
    "        # next_state is terminal - game ended after our move\n",
    "        elif self.is_terminal(next_state):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function loaded from the file.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"value_function_passive.pkl\", \"rb\") as f:\n",
    "    value_fun = pickle.load(f)\n",
    "    print('Value function loaded from the file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered length: 292495\n"
     ]
    }
   ],
   "source": [
    "filtered_value_function = {}\n",
    "for key in value_fun.keys():\n",
    "    reshaped_board = np.array(key).reshape(4, 5)\n",
    "    player1_count = np.sum(reshaped_board==1)\n",
    "    player2_count = np.sum(reshaped_board==2)\n",
    "        \n",
    "    if player1_count == player2_count:\n",
    "        filtered_value_function[key] = 0\n",
    "            \n",
    "print(f\"Filtered length: {len(filtered_value_function)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env = Connect3Environment(4, 5, filtered_value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, gamma, theta):\n",
    "    \"\"\"\n",
    "            This function calculate optimal policy for the specified MDP using Value Iteration approach:\n",
    "\n",
    "            'mdp' - model of the environment, use following functions:\n",
    "                get_all_states - return list of all states available in the environment\n",
    "                get_possible_actions - return list of possible actions for the given state\n",
    "                get_next_states - return list of possible next states with a probability for transition from state by taking\n",
    "                                  action into next_state\n",
    "                get_reward - return the reward after taking action in state and landing on next_state\n",
    "\n",
    "\n",
    "            'gamma' - discount factor for MDP\n",
    "            'theta' - algorithm should stop when minimal difference between previous evaluation of policy and current is\n",
    "                      smaller than theta\n",
    "            Function returns optimal policy and value function for the policy\n",
    "       \"\"\"\n",
    "    V = dict()\n",
    "    policy = dict()\n",
    "\n",
    "    # init with a policy with first avail action for each state\n",
    "    for current_state in mdp.get_all_states():\n",
    "        V[current_state] = 0\n",
    "        if len(mdp.get_possible_actions(current_state))>0:\n",
    "            policy[current_state] = mdp.get_possible_actions(current_state)[0]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    #\n",
    "    # INSERT CODE HERE to evaluate the best policy and value function for the given mdp\n",
    "    #\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in mdp.get_all_states():\n",
    "            v = V[s]\n",
    "            action_values = []\n",
    "            for action in mdp.get_possible_actions(s):\n",
    "                action_value = 0\n",
    "                for sprim in mdp.get_next_states(s, action):\n",
    "                    #print(V[sprim])\n",
    "                    action_value = action_value + mdp.get_next_states(s, action)[sprim] * (mdp.get_reward(s, action, sprim)+gamma*V.get(sprim, 0))\n",
    "                action_values.append([action_value, action])\n",
    "            \n",
    "            action_values.sort(key=lambda x: x[0], reverse=True)\n",
    "            \n",
    "            if len(action_values)>0:\n",
    "                delta = max(delta, abs(V[s] - action_values[0][0]))\n",
    "                V[s] = action_values[0][0]\n",
    "                policy[s] = action_values[0][1]\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        print(delta)\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9\n",
      "1.17\n",
      "0.405\n",
      "0.235\n",
      "0.03644999999999998\n",
      "0.0026818087500000143\n",
      "0.00034992000000000356\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, optimal_value = value_iteration(env, 0.9, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"passive_learning_policy.pkl\", \"wb\") as fw:\n",
    "    pickle.dump(optimal_policy, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"C:passive_learning_value_fun.pkl\", \"wb\") as fw:\n",
    "    pickle.dump(optimal_value, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
